{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "january_2022 = [0, 185.4, 0, 83, 0, 0, 458.69, 1459.26, 30, 9, 28, 21, 10, 79.94, 0, 246, 25.06, 7, 137.45, 92, 27, 51.89, 3.11, 27, 20, 0, 7, 0.45, 4.53, 39.02, 5.8] #31\n",
    "february_2022 = [0.2, 0, 15.99, 0.01, 0, 96, 4, 4.05, 14, 17, 12.10, 30, 0, 9, 0, 26, 24.29, 42, 11, 2.5, 2, 37, 10, 7, 0, 4, 5, 24] #28\n",
    "march_2022 = [36, 134.67, 16.29, 57, 51.3, 54.29, 35, 9.88, 153, 10, 20, 1433, 12, 7, 27, 102, 30, 24, 18, 3.5, 29.29, 0, 0, 4, 6, 1980, 0, 2, 0, 44, 0] #31\n",
    "april_2022 = [0, 0, 0, 651, 104.26, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 529.81, 42, 21, 2960.26, 43, 0, 25, 33, 5, 30, 0, 105, 0] #30\n",
    "may_2022 = [117, 0, 45, 10, 55.13, 70, 40, 75, 24, 18.2, 128, 23, 70, 25, 141.75, 33, 44, 22, 0, 15, 47, 42, 39, 0, 4, 0, 25, 24, 0, 15, 8] #31\n",
    "june_2022 = [35.00, 34, 0, 0, 17, 4, 0, 41, 92.7, 7, 0, 0, 5, 30.27, 11, 6, 17.1, 1053.52, 1, 945.83, 104, 0, 33, 0, 569, 1642.59, 592, 252.15, 0, 0] #30\n",
    "july_2022 = [54.95, 100.16, 35, 4, 19, 19, 7, 715.19, 18, 29, 0, 1439, 16, 5910.92, 31.78, 43, 106.17, 23.05, 8, 23.5, 0, 38, 41, 826.82, 1.55, 6, 15.5, 2149.86, 46, 0, 24] #31\n",
    "august_2022 = [19.5, 25, 29, 9, 2, 16, 249, 0, 514, 0, 294.25, 15, 130.25, 181.7, 3.5, 6.5, 0, 52.65, 20, 10.18, 487.99, 25, 0, 0, 805.82, 110, 42, 4, 46, 42.05, 0] #31\n",
    "september_2022 = [16, 19, 2523.41, 26, 0, 11, 16, 66, 8, 52.62, 34, 34, 212, 1359, 72, 0, 138.25, 165.55, 9, 0, 0, 0, 80, 471.3, 92.67, 74, 7, 12, 122, 2090.25] #30\n",
    "october_2022 = [39, 14, 10, 34, 14, 16, 23, 112.9, 0, 1288, 0, 29, 19, 11, 33, 46, 27, 30, 4, 11, 17, 0, 11, 4, 43, 530.24, 4, 62, 1, 3, 4] #31\n",
    "november_2022 = [35, 0, 101, 26, 76.5, 0, 23, 0, 4, 3, 16, 503.8, 17, 47, 40, 4, 3, 38, 0, 38, 124.71, 56, 4, 162, 75, 34, 0, 5, 7, 4] #30\n",
    "december_2022 = [23, 6, 4, 49, 24, 30, 17, 36, 19, 62, 88, 0, 1207, 0, 0, 0, 0, 14, 613.46, 0, 0, 0, 0, 0, 0, 1656.2, 73, 7, 366.38, 155.58, 122.5] #31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "amounts = pd.DataFrame(january_2022 + february_2022 + march_2022 + april_2022 + may_2022 + june_2022 + july_2022 + august_2022 + september_2022 + october_2022 + november_2022 + december_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10)).suptitle(\"Daily FB Price\", fontsize=20)\n",
    "# plt.plot(amounts)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the Data\n",
    "scaler = StandardScaler()\n",
    "scaled_amounts = amounts.values\n",
    "scaled_amounts = scaler.fit_transform(amounts.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_dataset(data, lookback=1):\n",
    "    data_x, data_y = [], []\n",
    "    for i in range(len(data)- lookback -1):\n",
    "            a = data[i:(i+ lookback), 0]\n",
    "            data_x.append(a)\n",
    "            data_y.append(data[i + lookback, 0])\n",
    "    return np.array(data_x), np.array(data_y)\n",
    "\n",
    "#lookback for 25 previous days\n",
    "lookback=7\n",
    "\n",
    "#Create X and Y for training\n",
    "train_x, train_y = create_rnn_dataset(scaled_amounts,lookback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 16:04:29.071122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 32)                1280      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,313\n",
      "Trainable params: 1,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-07 16:04:33.669024: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(3)\n",
    "\n",
    "# Create a Keras Model\n",
    "price_model = Sequential()\n",
    "# Add Simple RNN layer with 32 nodes\n",
    "price_model.add(SimpleRNN(32, input_shape=(1, lookback)))\n",
    "# Add a Dense layer at the end for output\n",
    "price_model.add(Dense(1))\n",
    "\n",
    "# Compile with Adam Optimizer. Optimize for minimum mean square error\n",
    "price_model.compile(loss=\"mean_squared_error\",\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=[\"mse\"])\n",
    "\n",
    "# Print model summary\n",
    "price_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "358/358 [==============================] - 1s 2ms/step - loss: 0.9428 - mse: 0.9428\n",
      "Epoch 2/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9431 - mse: 0.9431\n",
      "Epoch 3/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9391 - mse: 0.9391\n",
      "Epoch 4/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9317 - mse: 0.9317\n",
      "Epoch 5/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9297 - mse: 0.9297\n",
      "Epoch 6/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9282 - mse: 0.9282\n",
      "Epoch 7/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9279 - mse: 0.9279\n",
      "Epoch 8/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9225 - mse: 0.9225\n",
      "Epoch 9/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9204 - mse: 0.9204\n",
      "Epoch 10/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9189 - mse: 0.9189\n",
      "Epoch 11/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9190 - mse: 0.9190\n",
      "Epoch 12/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9146 - mse: 0.9146\n",
      "Epoch 13/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9077 - mse: 0.9077\n",
      "Epoch 14/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9118 - mse: 0.9118\n",
      "Epoch 15/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.9064 - mse: 0.9064\n",
      "Epoch 16/200\n",
      "358/358 [==============================] - 1s 2ms/step - loss: 0.9048 - mse: 0.9048\n",
      "Epoch 17/200\n",
      "358/358 [==============================] - 1s 1ms/step - loss: 0.9039 - mse: 0.9039\n",
      "Epoch 18/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8953 - mse: 0.8953\n",
      "Epoch 19/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8921 - mse: 0.8921\n",
      "Epoch 20/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8978 - mse: 0.8978\n",
      "Epoch 21/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8901 - mse: 0.8901\n",
      "Epoch 22/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8965 - mse: 0.8965\n",
      "Epoch 23/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8910 - mse: 0.8910\n",
      "Epoch 24/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8796 - mse: 0.8796\n",
      "Epoch 25/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8838 - mse: 0.8838\n",
      "Epoch 26/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8795 - mse: 0.8795\n",
      "Epoch 27/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8787 - mse: 0.8787\n",
      "Epoch 28/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8648 - mse: 0.8648\n",
      "Epoch 29/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8745 - mse: 0.8745\n",
      "Epoch 30/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8718 - mse: 0.8718\n",
      "Epoch 31/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8692 - mse: 0.8692\n",
      "Epoch 32/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8718 - mse: 0.8718\n",
      "Epoch 33/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8611 - mse: 0.8611\n",
      "Epoch 34/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8657 - mse: 0.8657\n",
      "Epoch 35/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8670 - mse: 0.8670\n",
      "Epoch 36/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8650 - mse: 0.8650\n",
      "Epoch 37/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8525 - mse: 0.8525\n",
      "Epoch 38/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8580 - mse: 0.8580\n",
      "Epoch 39/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8585 - mse: 0.8585\n",
      "Epoch 40/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8526 - mse: 0.8526\n",
      "Epoch 41/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8512 - mse: 0.8512\n",
      "Epoch 42/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8456 - mse: 0.8456\n",
      "Epoch 43/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8520 - mse: 0.8520\n",
      "Epoch 44/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8462 - mse: 0.8462\n",
      "Epoch 45/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8480 - mse: 0.8480\n",
      "Epoch 46/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8471 - mse: 0.8471\n",
      "Epoch 47/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8416 - mse: 0.8416\n",
      "Epoch 48/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8402 - mse: 0.8402\n",
      "Epoch 49/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8315 - mse: 0.8315\n",
      "Epoch 50/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8392 - mse: 0.8392\n",
      "Epoch 51/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8441 - mse: 0.8441\n",
      "Epoch 52/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8337 - mse: 0.8337\n",
      "Epoch 53/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8323 - mse: 0.8323\n",
      "Epoch 54/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8335 - mse: 0.8335\n",
      "Epoch 55/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8247 - mse: 0.8247\n",
      "Epoch 56/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8277 - mse: 0.8277\n",
      "Epoch 57/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8346 - mse: 0.8346\n",
      "Epoch 58/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8270 - mse: 0.8270\n",
      "Epoch 59/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8240 - mse: 0.8240\n",
      "Epoch 60/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8228 - mse: 0.8228\n",
      "Epoch 61/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8220 - mse: 0.8220\n",
      "Epoch 62/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8241 - mse: 0.8241\n",
      "Epoch 63/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8249 - mse: 0.8249\n",
      "Epoch 64/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8219 - mse: 0.8219\n",
      "Epoch 65/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8197 - mse: 0.8197\n",
      "Epoch 66/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8145 - mse: 0.8145\n",
      "Epoch 67/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8196 - mse: 0.8196\n",
      "Epoch 68/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8186 - mse: 0.8186\n",
      "Epoch 69/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8172 - mse: 0.8172\n",
      "Epoch 70/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8205 - mse: 0.8205\n",
      "Epoch 71/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8115 - mse: 0.8115\n",
      "Epoch 72/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8075 - mse: 0.8075\n",
      "Epoch 73/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8133 - mse: 0.8133\n",
      "Epoch 74/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8113 - mse: 0.8113\n",
      "Epoch 75/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8127 - mse: 0.8127\n",
      "Epoch 76/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8064 - mse: 0.8064\n",
      "Epoch 77/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8060 - mse: 0.8060\n",
      "Epoch 78/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8059 - mse: 0.8059\n",
      "Epoch 79/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8054 - mse: 0.8054\n",
      "Epoch 80/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7966 - mse: 0.7966\n",
      "Epoch 81/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8052 - mse: 0.8052\n",
      "Epoch 82/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8016 - mse: 0.8016\n",
      "Epoch 83/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8061 - mse: 0.8061\n",
      "Epoch 84/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7997 - mse: 0.7997\n",
      "Epoch 85/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7980 - mse: 0.7980\n",
      "Epoch 86/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8004 - mse: 0.8004\n",
      "Epoch 87/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.8043 - mse: 0.8043\n",
      "Epoch 88/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7955 - mse: 0.7955\n",
      "Epoch 89/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7870 - mse: 0.7870\n",
      "Epoch 90/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7947 - mse: 0.7947\n",
      "Epoch 91/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7973 - mse: 0.7973\n",
      "Epoch 92/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7925 - mse: 0.7925\n",
      "Epoch 93/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7863 - mse: 0.7863\n",
      "Epoch 94/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7894 - mse: 0.7894\n",
      "Epoch 95/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7917 - mse: 0.7917\n",
      "Epoch 96/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7864 - mse: 0.7864\n",
      "Epoch 97/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7854 - mse: 0.7854\n",
      "Epoch 98/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7889 - mse: 0.7889\n",
      "Epoch 99/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7936 - mse: 0.7936\n",
      "Epoch 100/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7809 - mse: 0.7809\n",
      "Epoch 101/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7839 - mse: 0.7839\n",
      "Epoch 102/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7850 - mse: 0.7850\n",
      "Epoch 103/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7857 - mse: 0.7857\n",
      "Epoch 104/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7861 - mse: 0.7861\n",
      "Epoch 105/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7918 - mse: 0.7918\n",
      "Epoch 106/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7812 - mse: 0.7812\n",
      "Epoch 107/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7798 - mse: 0.7798\n",
      "Epoch 108/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7796 - mse: 0.7796\n",
      "Epoch 109/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7788 - mse: 0.7788\n",
      "Epoch 110/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7751 - mse: 0.7751\n",
      "Epoch 111/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7724 - mse: 0.7724\n",
      "Epoch 112/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7843 - mse: 0.7843\n",
      "Epoch 113/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7775 - mse: 0.7775\n",
      "Epoch 114/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7724 - mse: 0.7724\n",
      "Epoch 115/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7769 - mse: 0.7769\n",
      "Epoch 116/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7701 - mse: 0.7701\n",
      "Epoch 117/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7771 - mse: 0.7771\n",
      "Epoch 118/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7740 - mse: 0.7740\n",
      "Epoch 119/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7728 - mse: 0.7728\n",
      "Epoch 120/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7750 - mse: 0.7750\n",
      "Epoch 121/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7723 - mse: 0.7723\n",
      "Epoch 122/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7711 - mse: 0.7711\n",
      "Epoch 123/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7610 - mse: 0.7610\n",
      "Epoch 124/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7632 - mse: 0.7632\n",
      "Epoch 125/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7729 - mse: 0.7729\n",
      "Epoch 126/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7712 - mse: 0.7712\n",
      "Epoch 127/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7639 - mse: 0.7639\n",
      "Epoch 128/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7681 - mse: 0.7681\n",
      "Epoch 129/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7710 - mse: 0.7710\n",
      "Epoch 130/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7670 - mse: 0.7670\n",
      "Epoch 131/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7596 - mse: 0.7596\n",
      "Epoch 132/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7651 - mse: 0.7651\n",
      "Epoch 133/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7623 - mse: 0.7623\n",
      "Epoch 134/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7603 - mse: 0.7603\n",
      "Epoch 135/200\n",
      "358/358 [==============================] - 1s 2ms/step - loss: 0.7595 - mse: 0.7595\n",
      "Epoch 136/200\n",
      "358/358 [==============================] - 1s 2ms/step - loss: 0.7550 - mse: 0.7550\n",
      "Epoch 137/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7598 - mse: 0.7598\n",
      "Epoch 138/200\n",
      "358/358 [==============================] - 1s 2ms/step - loss: 0.7538 - mse: 0.7538\n",
      "Epoch 139/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7719 - mse: 0.7719\n",
      "Epoch 140/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7651 - mse: 0.7651\n",
      "Epoch 141/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7540 - mse: 0.7540\n",
      "Epoch 142/200\n",
      "358/358 [==============================] - 1s 2ms/step - loss: 0.7628 - mse: 0.7628\n",
      "Epoch 143/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7559 - mse: 0.7559\n",
      "Epoch 144/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7596 - mse: 0.7596\n",
      "Epoch 145/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7522 - mse: 0.7522\n",
      "Epoch 146/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7557 - mse: 0.7557\n",
      "Epoch 147/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7611 - mse: 0.7611\n",
      "Epoch 148/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7546 - mse: 0.7546\n",
      "Epoch 149/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7513 - mse: 0.7513\n",
      "Epoch 150/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7497 - mse: 0.7497\n",
      "Epoch 151/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7496 - mse: 0.7496\n",
      "Epoch 152/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7531 - mse: 0.7531\n",
      "Epoch 153/200\n",
      "358/358 [==============================] - 1s 1ms/step - loss: 0.7440 - mse: 0.7440\n",
      "Epoch 154/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7518 - mse: 0.7518\n",
      "Epoch 155/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7505 - mse: 0.7505\n",
      "Epoch 156/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7548 - mse: 0.7548\n",
      "Epoch 157/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7400 - mse: 0.7400\n",
      "Epoch 158/200\n",
      "358/358 [==============================] - 1s 1ms/step - loss: 0.7431 - mse: 0.7431\n",
      "Epoch 159/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7527 - mse: 0.7527\n",
      "Epoch 160/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7470 - mse: 0.7470\n",
      "Epoch 161/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7416 - mse: 0.7416\n",
      "Epoch 162/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7486 - mse: 0.7486\n",
      "Epoch 163/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7401 - mse: 0.7401\n",
      "Epoch 164/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7416 - mse: 0.7416\n",
      "Epoch 165/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7447 - mse: 0.7447\n",
      "Epoch 166/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7388 - mse: 0.7388\n",
      "Epoch 167/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7355 - mse: 0.7355\n",
      "Epoch 168/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7368 - mse: 0.7368\n",
      "Epoch 169/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7380 - mse: 0.7380\n",
      "Epoch 170/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7402 - mse: 0.7402\n",
      "Epoch 171/200\n",
      "358/358 [==============================] - 1s 1ms/step - loss: 0.7429 - mse: 0.7429\n",
      "Epoch 172/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7378 - mse: 0.7378\n",
      "Epoch 173/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7418 - mse: 0.7418\n",
      "Epoch 174/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7345 - mse: 0.7345\n",
      "Epoch 175/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7387 - mse: 0.7387\n",
      "Epoch 176/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7344 - mse: 0.7344\n",
      "Epoch 177/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7306 - mse: 0.7306\n",
      "Epoch 178/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7344 - mse: 0.7344\n",
      "Epoch 179/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7350 - mse: 0.7350\n",
      "Epoch 180/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7288 - mse: 0.7288\n",
      "Epoch 181/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7315 - mse: 0.7315\n",
      "Epoch 182/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7336 - mse: 0.7336\n",
      "Epoch 183/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7310 - mse: 0.7310\n",
      "Epoch 184/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7270 - mse: 0.7270\n",
      "Epoch 185/200\n",
      "358/358 [==============================] - 1s 1ms/step - loss: 0.7333 - mse: 0.7333\n",
      "Epoch 186/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7239 - mse: 0.7239\n",
      "Epoch 187/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.6999 - mse: 0.6999\n",
      "Epoch 188/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7433 - mse: 0.7433\n",
      "Epoch 189/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7274 - mse: 0.7274\n",
      "Epoch 190/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7248 - mse: 0.7248\n",
      "Epoch 191/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7262 - mse: 0.7262\n",
      "Epoch 192/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7241 - mse: 0.7241\n",
      "Epoch 193/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7270 - mse: 0.7270\n",
      "Epoch 194/200\n",
      "358/358 [==============================] - 1s 1ms/step - loss: 0.7293 - mse: 0.7293\n",
      "Epoch 195/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7272 - mse: 0.7272\n",
      "Epoch 196/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7152 - mse: 0.7152\n",
      "Epoch 197/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7179 - mse: 0.7179\n",
      "Epoch 198/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7248 - mse: 0.7248\n",
      "Epoch 199/200\n",
      "358/358 [==============================] - 1s 1ms/step - loss: 0.7227 - mse: 0.7227\n",
      "Epoch 200/200\n",
      "358/358 [==============================] - 0s 1ms/step - loss: 0.7119 - mse: 0.7119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe324636be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_model.fit(train_x, train_y, epochs=200, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisMonthSoFar = np.array([76.5, 0, 53, 27, 63, 10, 0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_thisMonthSoFar = scaler.transform(thisMonthSoFar.reshape(-1, 1))\n",
    "model_input = np.reshape(scaled_thisMonthSoFar, (scaled_thisMonthSoFar.shape[1], 1, scaled_thisMonthSoFar.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential/simple_rnn/TensorArrayUnstack/TensorListFromTensor' defined at (most recent call last):\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 423, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n      return runner(coro)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/bf/9g9n0_vd74s0bv5js4v9_l440000gn/T/ipykernel_50989/492753127.py\", line 1, in <module>\n      prediction = price_model.predict(model_input)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/layers/rnn/base_rnn.py\", line 556, in __call__\n      return super().__call__(inputs, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/layers/rnn/simple_rnn.py\", line 410, in call\n      return super().call(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/layers/rnn/base_rnn.py\", line 722, in call\n      last_output, outputs, states = backend.rnn(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/backend.py\", line 4942, in rnn\n      input_ta = tuple(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/backend.py\", line 4943, in <genexpr>\n      ta.unstack(input_)\nNode: 'sequential/simple_rnn/TensorArrayUnstack/TensorListFromTensor'\nSpecified a list with shape [?,7] from a tensor with shape [1,8]\n\t [[{{node sequential/simple_rnn/TensorArrayUnstack/TensorListFromTensor}}]] [Op:__inference_predict_function_326829]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prediction \u001b[39m=\u001b[39m price_model\u001b[39m.\u001b[39;49mpredict(model_input)\n\u001b[1;32m      2\u001b[0m norm_prediction \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39minverse_transform(prediction)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/everything/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential/simple_rnn/TensorArrayUnstack/TensorListFromTensor' defined at (most recent call last):\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 423, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n      return runner(coro)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/bf/9g9n0_vd74s0bv5js4v9_l440000gn/T/ipykernel_50989/492753127.py\", line 1, in <module>\n      prediction = price_model.predict(model_input)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2350, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2137, in predict_function\n      return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2123, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in run_step\n      outputs = model.predict_step(data)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 2079, in predict_step\n      return self(x, training=False)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/functional.py\", line 668, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/layers/rnn/base_rnn.py\", line 556, in __call__\n      return super().__call__(inputs, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/layers/rnn/simple_rnn.py\", line 410, in call\n      return super().call(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/layers/rnn/base_rnn.py\", line 722, in call\n      last_output, outputs, states = backend.rnn(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/backend.py\", line 4942, in rnn\n      input_ta = tuple(\n    File \"/opt/anaconda3/envs/everything/lib/python3.9/site-packages/keras/backend.py\", line 4943, in <genexpr>\n      ta.unstack(input_)\nNode: 'sequential/simple_rnn/TensorArrayUnstack/TensorListFromTensor'\nSpecified a list with shape [?,7] from a tensor with shape [1,8]\n\t [[{{node sequential/simple_rnn/TensorArrayUnstack/TensorListFromTensor}}]] [Op:__inference_predict_function_326829]"
     ]
    }
   ],
   "source": [
    "prediction = price_model.predict(model_input)\n",
    "norm_prediction = scaler.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[239.73747]]\n"
     ]
    }
   ],
   "source": [
    "print(norm_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "everything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a861135c452baff45f2a415038bfc8edb6e3af747b14315b411ad5a559647f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
